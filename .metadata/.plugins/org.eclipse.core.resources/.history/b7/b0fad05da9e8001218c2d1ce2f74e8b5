package LDA.MAIN.INERENCE;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.FileWriter;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;


import LDA.MAIN.INERENCE.Documents;

/**
 * @author Eric
 * this class is work for LdaInference
 */


public class LdaModel2
{
	double[][] phi;/*这个是之前训练得到的*/
	ArrayList<String> indexToTermMap;/*这是之前训练得到的*/
	
	
	int[][] doc;// word index array
	int V, K, M;// vocabulary size, topic number, document number
	int[][] z;// topic label array
	float alpha; // doc-topic dirichlet prior parameter
	float beta; // topic-word dirichlet prior parameter
	int[][] nmk;// given document m, count times of topic k. M*K
	int[][] nkt;// given topic k, count times of term t. K*V
	int[] nmkSum;// Sum for each row in nmk
	int[] nktSum;// Sum for each row in nkt
	double[][] theta;// Parameters for doc-topic distribution M*K
	int iterations;// Times of iterations
	int saveStep;// The number of iterations between two saving
	int beginSaveIters;// Begin save model at this iteration
	
	public LdaModel2(LdaInference.InferenceParameters inferenceParameters,String phiPathName,String phiFileName) throws FileNotFoundException, IOException, ClassNotFoundException
	{
		this.alpha=inferenceParameters.alpha;
		this.beta=inferenceParameters.beta;
		this.iterations=inferenceParameters.iteration;
		this.saveStep=inferenceParameters.saveStep;
		this.beginSaveIters=inferenceParameters.beginSaveIters;
		
		File file=new File(phiPathName,phiFileName);
		ObjectInputStream ois=new ObjectInputStream(new FileInputStream(file));
		phi=(double[][])ois.readObject();
		
		
	}
	
	public void initializeModel(Documents documents)
	{
		indexToTermMap=documents.indexToTermMap;
		
		M = documents.docs.size();
		V = documents.indexToTermMap.size();
		nmk = new int[M][K];
		nkt = new int[K][V];
		theta = new double[M][K];
		nmkSum = new int[M];
		nktSum = new int[K];

		doc = new int[M][];
		for (int i = 0; i < M; i++)
		{
			int N = documents.docs.get(i).docWords.length;
			doc[i] = new int[N];
			for (int j = 0; j < N; j++)
			{
				doc[i][j] = documents.docs.get(i).docWords[j];
			}
		}

		// random to z
		z = new int[M][];
		for (int i = 0; i < M; i++)
		{
			int N = documents.docs.get(i).docWords.length;
			z[i] = new int[N];
			for (int j = 0; j < N; j++)
			{
				int initTopic = (int) (Math.random() * K);
				int termindexInM = documents.docs.get(i).docWords[j];
				z[i][j] = initTopic;
				nmk[i][initTopic] += 1;
				nkt[initTopic][termindexInM] += 1;
				nmkSum[i]++;
				nktSum[initTopic]++;
			}
		}
	}
	
	
	public void inferenceModel(Documents documents, String reusltPath)
			throws IOException
	{
		if (beginSaveIters + saveStep > iterations)
		{
			System.err
					.println("Error: the number of iterations should be larger than "
							+ (beginSaveIters + saveStep));
			System.exit(0);
		}

		for (int i = 0; i < iterations; i++)
		{
			// updatePara and save the model
			if (i >= beginSaveIters && (i - beginSaveIters) % saveStep == 0)
			{
				System.out.println("save model at iterations " + i + "...");
				updateEstimateParameter();
				saveIteratedModel(i, documents, reusltPath);
			}

			// gibbs sampling to update z[][]
			for (int m = 0; m < M; m++)
			{
				int N = documents.docs.get(m).docWords.length; 
				for (int n = 0; n < N; n++)
				{
					int topic = sampleTopicZ(m, n);
					z[m][n] = topic;
				}
			}
		}
	}
	
	
	public void updateEstimateParameter()
	{
		// wo use the average of p instead of maxlikelihood
		for (int m = 0; m < M; m++)
		{
			for (int k = 0; k < K; k++)
			{
				theta[m][k] = (nmk[m][k] + alpha) / (nmkSum[m] + K * alpha);
			}
		}
	}
	
	public int sampleTopicZ(int m, int n)
	{
		int oldTopic = z[m][n];
		nmk[m][oldTopic]--;
		int indexInV = doc[m][n];
		nkt[oldTopic][indexInV]--;
		nktSum[oldTopic]--;
		nmkSum[m]--;

		double[] p = new double[K];
		for (int k = 0; k < K; k++)
		{
			p[k] = (nmk[m][k] + alpha) / (nmkSum[m] + K * alpha)
					* (nkt[k][indexInV] + beta) / (nktSum[k] + V * beta);
		}

		// wo get the probability distribution of K, i.e.,the jump probability
		// now wo sample the topic wo jump to
		double sumPk = 0.0;
		for (int k = 0; k < K; k++)
		{
			sumPk += p[k];
		}
		double u = Math.random() * sumPk;
		int newTopic;
		for (newTopic = 0; newTopic < K; newTopic++)
		{
			if (p[newTopic] > u)
				break;
		}
		if (newTopic != K)
		{
			nmk[m][newTopic]++;
			nmkSum[m]++;
			nkt[newTopic][indexInV]++;
			nktSum[newTopic]++;
			return newTopic;
		}
		else
		{
			nmk[m][oldTopic]++;
			nmkSum[m]++;
			nkt[oldTopic][indexInV]++;
			nktSum[oldTopic]++;
			return oldTopic;
		}
	}

	
	
	public void saveIteratedModel(int iters, Documents docSet, String resultPath)
			throws IOException
	{
		String modelName = "lda_" + iters;
		BufferedWriter writer = new BufferedWriter(new FileWriter(resultPath
				+ modelName + ".theta"));
		
		for (int i = 0; i < M; i++)
		{
			List<Integer> tWordsIndexArray = new ArrayList<Integer>();
			for (int j = 0; j < K; j++)
			{
				tWordsIndexArray.add(new Integer(j));
			}
			
			writer.write("topic " + i + "\n");
			for (int t = 0; t < topNum; t++)
			{
				writer.write(docSet.indexToTermMap.get(tWordsIndexArray.get(t))
						+ " " + phi[i][tWordsIndexArray.get(t)] + "\t");
			}
			writer.write("\n\n");
		}
		writer.close();
	}
}
